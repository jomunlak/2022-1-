


from sklearn.linear_model import LinearRegression
model = LinearRegression() # 비어있는 선형 회귀 모델
model.fit(X, y)      # 모델에 데이터를 훈련시키는 코드

y_pred = model.predict(X) # 실제 훈련이 되었는지 훈련 시킨 데이터로 다시 예측을 시켜봄


  선형회귀의 방정식
 ** y = w1x1 + [w2x2 + w3x3 +...] + b 형태의 식.  **    

W = model.coef_        # w : 직선의 기울기(가중치), x가 여러개일경우 각각의 가중치가 다르게 설정 할 수 있다.     
b = model.intercept_   # b : 절편
display(W, b)          # 입력 x와 출력 y가 정해져있기 때문에 w와 b를 정하는게 가장 중요한 문제가 된다.
                       # 위의 fit 함수에 X와 y를 입력해준순간 W와 b가 알아서 계산됨. 그 결과가 저장된 변수가 model.coef_, model.intercept_


# 속성을 여러개 쓰는 선형 회귀 모델의 경우
X = iris.data[:,:3] # 0~2 칼럼을 입력값으로 설정
y = iris.data[:,3]  # 3번 칼럼을 결과로 설정

model = LinearRegression()
model.fit(X, y)

display(model.coef_, model.intercept_)
>>>
array([-0.20726607,  0.22282854,  0.52408311])
-0.24030738911226113

속성이 여러개인 경우에는 그림을 그릴 수가 없다... 따라서 선형 회귀의 경우에는 속성이 하나인 경우가 많다.

선형 회귀는 가장 간단한 신경망이다.


회귀의 기초가 선형회귀였다면 분류의 기초는 로지스틱회귀이다.(이름만 회귀이고 분류 알고리즘이다.)
y = sigmoid(w1x1 + w2x2 + w3x3 +...+ b) >>> 선형 회귀 알고리즘에 sigmoid함수만 적용한 알고리즘
				     >>> 출력되는 값은 0~ 1 사이의 값. >> 0~1 사이의 값은 확률이라고 할 수 있다.


활성화 함수의 유무가 회귀냐 분류냐를 나눈다?


# 분류의 기초 - 로지스틱 회귀

X = iris.data[:,:2]
y = np.array([0]*50 + [1]*100) # 0이라면 setosa, 1이라면 setosa가 아니라고 알려줌

from sklearn.linear_model import LogisticRegression

model = LogisticRegression()
model.fit(X, y)

y_pred = model.predict(X)

w = model.coef_[0]
b = model.intercept_[0]


xs = np.array([4, 8])
ys = -(w[0]*xs + b)/w[1]     # w1x1 + w2x2 + b = 0 >>>> w1x1+b = -w2x2  >>>>> -(w1x1 + b)/w2 = x2  
                              #   이 방정식 위 양의 값을 가지면 1, 아니면 0
                               # 의문점, 정확히 이 실선에 일치하는 데이터는 어떻게 판단하는가?


# 그래프의 y축은 iris.data의 0번째 속성, y축은 1번째 속성 
plt.scatter(X[:,0], X[:,1] , c=y)
plt.plot(xs, ys, 'r')



sigmoid함수의 입력으로 양수 혹은 음수가(-(w1x1+ b)/w2 = x2 그래프의 위에 있는지, 아래에 있는지) 
들어가고, 출력으로 입력이 양수라면 0.5 이상인 수치가, 입력이 음수라면 0.5 이하인 수치가 나온다.

다중분류 : 클래스가 3개 이상인 경우
-로지스틱 회귀에서 기본적으로는 2개의 클래스만 구분한다
-다중 분류일 경우 소프트맥스(softmax) 함수를 적용한 일대다 방식을 적용한다.


# 모든 속성을 사용해서 모든 종류의 꽃을 분류하는 경우

X = iris.data
y = iris.target

model = LogisticRegression()
model.fit(X, y)
y_pred = model.predict(X)

display(y, y_pred)

result : 
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
       1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1,
       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 1, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,
       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])

model.score(X, y)

0.9733333333333334


(y==y_pred).sum()
146 >true는 1, false는 0으로 처리하기 때문에 일치하는 항목이 146개라는걸 알 수 있다
(y!=y_pred).sum()
4 >> 반대로 적용하면 4개가 이리치하지 않는다는 결과가 나온다.

np.where(y!=y_pred)  *******
(array([ 70,  77,  83, 106], dtype=int64),)  >>>> false의 위치를 알 수 있다.

y[[np.where(y!=y_pred)]]
array([[1, 1, 1, 2]]) >>>> false의 위치를 알면 그 값도 알 수 있다.
